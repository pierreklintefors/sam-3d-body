{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment\n",
    "import sys\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone repository if not already present\n",
    "if not os.path.exists('/content/sam-3d-body'):\n",
    "    !git clone https://github.com/facebookresearch/sam-3d-body.git /content/sam-3d-body\n",
    "\n",
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install -r /content/sam-3d-body/requirements.txt\n",
    "# !pip install -e /content/sam-3d-body\n",
    "\n",
    "# Add repo to python path\n",
    "sys.path.append('/content/sam-3d-body')\n",
    "os.chdir('/content/sam-3d-body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sam_3d_body import load_sam_3d_body, SAM3DBodyEstimator\n",
    "from tools.vis_utils import visualize_sample_together\n",
    "from tools.build_detector import HumanDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration\n",
    "# Define paths to your model and assets in Google Drive\n",
    "checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/PoseEstimation/SAM-Body3d/model.ckpt\"\n",
    "mhr_path = \"/content/drive/MyDrive/Colab Notebooks/PoseEstimation/SAM-Body3d/assets/mhr_model.pt\"\n",
    "\n",
    "# Define input and output video paths\n",
    "# Replace with your actual video path in Drive or upload one\n",
    "video_path = \"/content/drive/MyDrive/Colab Notebooks/PoseEstimation/SAM-Body3d/input_video.mp4\"\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/PoseEstimation/SAM-Body3d/output_video.mp4\"\n",
    "\n",
    "# Optional: Path to model_config.yaml if it's not automatically found near the checkpoint\n",
    "# model_config_path = \"/content/drive/MyDrive/Colab Notebooks/PoseEstimation/SAM-Body3d/model_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load Model\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the SAM 3D Body model\n",
    "# We pass the paths directly. If you have a specific config path, pass model_config_path=model_config_path\n",
    "model, model_cfg = load_sam_3d_body(\n",
    "    checkpoint_path, \n",
    "    device=device, \n",
    "    mhr_path=mhr_path\n",
    ")\n",
    "\n",
    "# Initialize Human Detector (ViTDet)\n",
    "print(\"Loading Human Detector...\")\n",
    "human_detector = HumanDetector(name=\"vitdet\", device=device)\n",
    "\n",
    "# Create the Estimator\n",
    "estimator = SAM3DBodyEstimator(\n",
    "    sam_3d_body_model=model,\n",
    "    model_cfg=model_cfg,\n",
    "    human_detector=human_detector,\n",
    "    # human_segmentor=None, # Add segmentor if needed\n",
    "    # fov_estimator=None,   # Add FOV estimator if needed\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Process Video\n",
    "def process_video_colab(estimator, video_path, output_path):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Output will be saved to: {output_path}\")\n",
    "    \n",
    "    # Use the optimized generator (process_video) which avoids clearing cache every frame\n",
    "    generator = estimator.process_video(\n",
    "        video_path,\n",
    "        bbox_thr=0.5,\n",
    "        use_mask=False,\n",
    "        verbose=False \n",
    "    )\n",
    "    \n",
    "    for frame, outputs in tqdm(generator, total=total_frames):\n",
    "        # Visualize results on the frame\n",
    "        rend_img = visualize_sample_together(frame, outputs, estimator.faces)\n",
    "        writer.write(rend_img.astype(np.uint8))\n",
    "        \n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    print(\"Done.\")\n",
    "\n",
    "# Run the processing\n",
    "process_video_colab(estimator, video_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (Optional) Download or View Result\n",
    "# If running in Colab, you can download the file to your local machine\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab or files module not available.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
